<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Introduction to PyTorch | Duc Minh Quang Nguyen</title> <meta name="author" content="Duc Minh Quang Nguyen"> <meta name="description" content="# A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://nminhquang380.github.io/myblog/2023/intro_pytorch/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Duc Minh Quang </span>Nguyen</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Portfolio</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">Resume</a> </li> <li class="nav-item "> <a class="nav-link" href="/bucket_list/">My Bucket List</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Introduction to PyTorch</h1> <p class="post-meta">August 31, 2023</p> <p class="post-tags"> <i class="fa-solid fa-calendar fa-sm"></i> 2023   ·   <i class="fa-solid fa-tag fa-sm"></i> machinelearning   </p> </header> <article class="post-content"> <div id="markdown-content"> <h2 id="tensors">Tensors</h2> <p>Tensors are a specialized data structure that are very similar to arrays and matrices. In PyTorch, we use tensors to encode the inputs and outputs of a model, as well as the model’s parameters.</p> <p>Tensors are similar to NumPy’s ndarrays, except that tensors can run on GPUs or other specialized hardware to accelerate computing.</p> <h3 id="initialization">Initialization</h3> <ul> <li> <p>Directly from data</p> <div class="language-python highlighter-rouge"> <div class="highlight"><pre class="highlight"><code>  <span class="n">data</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]]</span>
  <span class="n">x_data</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
</code></pre></div> </div> </li> <li> <p>From numpy array</p> <div class="language-python highlighter-rouge"> <div class="highlight"><pre class="highlight"><code>  <span class="n">np_array</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
  <span class="n">x_np</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">from_numpy</span><span class="p">(</span><span class="n">np_array</span><span class="p">)</span> <span class="c1"># note
</span></code></pre></div> </div> </li> <li> <p>From another tensor</p> <div class="language-python highlighter-rouge"> <div class="highlight"><pre class="highlight"><code>  <span class="n">x_ones</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">ones_like</span><span class="p">(</span><span class="n">x_data</span><span class="p">)</span> <span class="c1"># retains the properties of x_data
</span>  <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Ones Tensor: </span><span class="se">\n</span><span class="s"> </span><span class="si">{</span><span class="n">x_ones</span><span class="si">}</span><span class="s"> </span><span class="se">\n</span><span class="sh">"</span><span class="p">)</span>
    
  <span class="n">x_rand</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">rand_like</span><span class="p">(</span><span class="n">x_data</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">float</span><span class="p">)</span> <span class="c1"># overrides the datatype of x_data
</span>  <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Random Tensor: </span><span class="se">\n</span><span class="s"> </span><span class="si">{</span><span class="n">x_rand</span><span class="si">}</span><span class="s"> </span><span class="se">\n</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div> </div> </li> <li> <p>With random or constant values</p> <div class="language-python highlighter-rouge"> <div class="highlight"><pre class="highlight"><code>  <span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,)</span>
  <span class="n">rand_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
  <span class="n">ones_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
  <span class="n">zeros_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
    
  <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Random Tensor: </span><span class="se">\n</span><span class="s"> </span><span class="si">{</span><span class="n">rand_tensor</span><span class="si">}</span><span class="s"> </span><span class="se">\n</span><span class="sh">"</span><span class="p">)</span>
  <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Ones Tensor: </span><span class="se">\n</span><span class="s"> </span><span class="si">{</span><span class="n">ones_tensor</span><span class="si">}</span><span class="s"> </span><span class="se">\n</span><span class="sh">"</span><span class="p">)</span>
  <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Zeros Tensor: </span><span class="se">\n</span><span class="s"> </span><span class="si">{</span><span class="n">zeros_tensor</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div> </div> </li> </ul> <h3 id="tensor-attributes">Tensor Attributes</h3> <p>Tensor attributes describe their shape, datatype, and the device on which they are stored.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Shape of tensor: </span><span class="si">{</span><span class="n">tensor</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Datatype of tensor: </span><span class="si">{</span><span class="n">tensor</span><span class="p">.</span><span class="n">dtype</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Device tensor is stored on: </span><span class="si">{</span><span class="n">tensor</span><span class="p">.</span><span class="n">device</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <h3 id="tensor-operations">Tensor Operations</h3> <p>Some tensor operations</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Returns the total number of elements in the input tensor.
</span><span class="n">tensor</span><span class="p">.</span><span class="nf">numel</span><span class="p">()</span>

<span class="c1"># Returns True if obj is a PyTorch tensor.
</span><span class="n">tensor</span><span class="p">.</span><span class="nf">is_tensor</span><span class="p">()</span>

<span class="c1"># CREATE TENSOR OPTIONS
# Constructs a tensor with no autograd history 
# (also known as a "leaf tensor", see Autograd mechanics) by copying data.
</span><span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">],[</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">]])</span>

<span class="c1"># Converts obj to a tensor.
</span><span class="n">arr</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">])</span>
<span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">asarray</span><span class="p">(</span><span class="n">arr</span><span class="p">)</span>

<span class="c1"># Creates a tensor from np.ndarray
</span><span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">from_numpy</span><span class="p">(</span><span class="n">arr</span><span class="p">)</span>

<span class="c1"># Returns a tensor filled with the scalar value 0, 
# with the shape defined by the variable argument size.
</span><span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="mi">4</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>

<span class="c1"># Returns a tensor filled with the scalar value 0, with the same size as input.
</span><span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros_like</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>

<span class="sh">"""</span><span class="s">
The same applies to ones, ones_like
</span><span class="sh">"""</span>

<span class="c1"># Returns a 1-D tensor of size [(end-start)/step] 
# with values from the interval [start, end) 
# taken with common difference step beginning from start.
</span><span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">start</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Creates a one-dimensional tensor of size steps whose values are evenly spaced from
# base^start to base^end , inclusive, on a logarithmic scale with base base.
</span><span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">logspace</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">end</span><span class="p">,</span> <span class="n">steps</span><span class="p">,</span> <span class="n">base</span><span class="o">=</span><span class="mf">10.0</span><span class="p">)</span>

<span class="c1"># Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.
</span><span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">eye</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>

<span class="c1"># Creates a tensor of size size filled with fill_value. 
# The tensor’s dtype is inferred from fill_value.
</span><span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">full</span><span class="p">((</span><span class="mi">4</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> <span class="mi">5</span><span class="p">)</span>
</code></pre></div></div> <ul> <li> <p><strong>Standard numpy-like indexing and slicing:</strong></p> <div class="language-python highlighter-rouge"> <div class="highlight"><pre class="highlight"><code>  <span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span>
  <span class="n">tensor</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
</code></pre></div> </div> </li> <li> <p><strong>Joining Tensor</strong></p> <div class="language-python highlighter-rouge"> <div class="highlight"><pre class="highlight"><code>  <span class="n">t1</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">([</span><span class="n">tensor</span><span class="p">,</span> <span class="n">tensor</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div> </div> </li> <li> <p><strong>Multiplying Tensors</strong></p> <div class="language-python highlighter-rouge"> <div class="highlight"><pre class="highlight"><code>  <span class="c1"># This computes the element-wise product
</span>  <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">tensor.mul(tensor) </span><span class="se">\n</span><span class="s"> </span><span class="si">{</span><span class="n">tensor</span><span class="p">.</span><span class="nf">mul</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span><span class="si">}</span><span class="s"> </span><span class="se">\n</span><span class="sh">"</span><span class="p">)</span>
  <span class="c1"># Alternative syntax:
</span>  <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">tensor * tensor </span><span class="se">\n</span><span class="s"> </span><span class="si">{</span><span class="n">tensor</span> <span class="o">*</span> <span class="n">tensor</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div> </div> </li> <li> <p>Matrix Multiply between 2 tensors</p> <div class="language-python highlighter-rouge"> <div class="highlight"><pre class="highlight"><code>  <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">tensor.matmul(tensor.T) </span><span class="se">\n</span><span class="s"> </span><span class="si">{</span><span class="n">tensor</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">tensor</span><span class="p">.</span><span class="n">T</span><span class="p">)</span><span class="si">}</span><span class="s"> </span><span class="se">\n</span><span class="sh">"</span><span class="p">)</span>
  <span class="c1"># Alternative syntax:
</span>  <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">tensor @ tensor.T </span><span class="se">\n</span><span class="s"> </span><span class="si">{</span><span class="n">tensor</span> <span class="o">@</span> <span class="n">tensor</span><span class="p">.</span><span class="n">T</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div> </div> </li> <li> <p><strong>In-place operations Operations that have a <code class="language-plaintext highlighter-rouge">_</code> suffix are in-place. For example: <code class="language-plaintext highlighter-rouge">x.copy_(y)</code>, <code class="language-plaintext highlighter-rouge">x.t_()</code>, will change <code class="language-plaintext highlighter-rouge">x</code>.</strong></p> <div class="language-python highlighter-rouge"> <div class="highlight"><pre class="highlight"><code>  <span class="nf">print</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="sh">"</span><span class="se">\n</span><span class="sh">"</span><span class="p">)</span>
  <span class="n">tensor</span><span class="p">.</span><span class="nf">add_</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
  <span class="nf">print</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>
</code></pre></div> </div> </li> </ul> <h2 id="introduction-to-torchautograd">Introduction to <code class="language-plaintext highlighter-rouge">torch.autograd</code> </h2> <p><code class="language-plaintext highlighter-rouge">torch.autograd</code> is PyTorch’s automatic differentiation engine that powers neural network training. In this section, you will get a conceptual understanding of how autograd helps a neural network train.</p> <h3 id="background">Background</h3> <p><strong>Neural networks</strong> (NNs) are a collection of nested functions that are executed on some input data. These functions are defined by <em>parameters</em> (consisting of weights and biases), which in PyTorch are stored in tensors.</p> <p>Training a NN happens in two steps:</p> <p><strong>Forward Propagation:</strong> In forward prop, the NN makes its best guess about the correct output. It runs the input data through each of its functions to make this guess.</p> <p><strong>Backward Propagation</strong>: In backprop, the NN adjusts its parameters proportionate to the error in its guess. It does this by traversing backwards from the output, collecting the derivatives of the error with respect to the parameters of the functions (<em>gradients</em>), and optimizing the parameters using gradient descent.</p> <h3 id="usage-in-pytorch">Usage in PyTorch</h3> <p><strong>Let’s take a look at a single training step. For this example, we load a pretrained resnet18 model from <code class="language-plaintext highlighter-rouge">torchvision</code>. We create a random data tensor to represent a single image with 3 channels, and height &amp; width of 64, and its corresponding <code class="language-plaintext highlighter-rouge">label</code> initialized to some random values. Label in pretrained models has shape (1,1000).</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">from</span> <span class="n">torchvision.models</span> <span class="kn">import</span> <span class="n">resnet18</span><span class="p">,</span> <span class="n">ResNet18_Weights</span>
<span class="n">model</span> <span class="o">=</span> <span class="nf">resnet18</span><span class="p">(</span><span class="n">weights</span><span class="o">=</span><span class="n">ResNet18_Weights</span><span class="p">.</span><span class="n">DEFAULT</span><span class="p">)</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
</code></pre></div></div> <p><strong>Next, we run the input data through the model through each of its layers to make a prediction. This is the forward pass.</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">prediction</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">data</span><span class="p">)</span> <span class="c1"># forward pass
</span></code></pre></div></div> <p><strong>We use the model’s prediction and the corresponding label to calculate the error (<code class="language-plaintext highlighter-rouge">loss</code>). The next step is to backpropagate this error through the network. Backward propagation is kicked off when we call <code class="language-plaintext highlighter-rouge">.backward()</code> on the error tensor. Autograd then calculates and stores the gradients for each model parameter in the parameter’s <code class="language-plaintext highlighter-rouge">.grad</code> attribute.</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">prediction</span> <span class="o">-</span> <span class="n">labels</span><span class="p">).</span><span class="nf">sum</span><span class="p">()</span>
<span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span> <span class="c1"># backward pass
</span></code></pre></div></div> <p><strong>Next, we load an optimizer, in this case SGD with a learning rate of 0.01 and <a href="https://towardsdatascience.com/stochastic-gradient-descent-with-momentum-a84097641a5d" rel="external nofollow noopener" target="_blank">momentum</a> of 0.9. We register all the parameters of the model in the optimizer.</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">optim</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">SGD</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-2</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
</code></pre></div></div> <p><strong>Finally, we call <code class="language-plaintext highlighter-rouge">.step()</code> to initiate gradient descent. The optimizer adjusts each parameter by its gradient stored in <code class="language-plaintext highlighter-rouge">.grad</code>.</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">optim</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>
</code></pre></div></div> <h2 id="neural-networks">Neural Networks</h2> <p><strong>Neural networks can be constructed using the <code class="language-plaintext highlighter-rouge">torch.nn</code> package.</strong></p> <p><strong>Now that you had a glimpse of <code class="language-plaintext highlighter-rouge">autograd</code>, <code class="language-plaintext highlighter-rouge">nn</code> depends on <code class="language-plaintext highlighter-rouge">autograd</code> to define models and differentiate them. An <code class="language-plaintext highlighter-rouge">nn.Module</code> contains layers, and a method <code class="language-plaintext highlighter-rouge">forward(input)</code> that returns the <code class="language-plaintext highlighter-rouge">output</code>.</strong></p> <p><strong>For example, look at this network that classifies digit images:</strong></p> <p><img src="Deep%20Learning%20with%20PyTorch%202e8f246cbce841a89a9ac2863858394c/Untitled.png" alt="Untitled"></p> <p><code class="language-plaintext highlighter-rouge">convnet</code></p> <p>It is a simple feed-forward network. It takes the input, feeds it through several layers one after the other, and then finally gives the output.</p> <p>A typical training procedure for a neural network is as follows:</p> <ul> <li>Define the neural network that has some learnable parameters (or weights)</li> <li>Iterate over a dataset of inputs</li> <li>Process input through the network</li> <li>Compute the loss (how far is the output from being correct)</li> <li>Propagate gradients back into the network’s parameters</li> <li>Update the weights of the network, typically using a simple update rule: <code class="language-plaintext highlighter-rouge">weight = weight - learning_rate * gradient</code> </li> </ul> <h3 id="define-the-network">Define the network</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="n">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span>

<span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="c1"># 1 input image channel, 6 output channels, 5x5 square convolution
</span>        <span class="c1"># kernel
</span>        <span class="n">self</span><span class="p">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
        <span class="c1"># an affine operation: y = Wx + b
</span>        <span class="n">self</span><span class="p">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">16</span> <span class="o">*</span> <span class="mi">5</span> <span class="o">*</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">120</span><span class="p">)</span>  <span class="c1"># 5*5 from image dimension
</span>        <span class="n">self</span><span class="p">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">120</span><span class="p">,</span> <span class="mi">84</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">fc3</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">84</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># Max pooling over a (2, 2) window
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">max_pool2d</span><span class="p">(</span><span class="n">F</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
        <span class="c1"># If the size is a square, you can specify with a single number
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">max_pool2d</span><span class="p">(</span><span class="n">F</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">)),</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">flatten</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="c1"># flatten all dimensions except the batch dimension
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">fc3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

<span class="n">net</span> <span class="o">=</span> <span class="nc">Net</span><span class="p">()</span>
<span class="nf">print</span><span class="p">(</span><span class="n">net</span><span class="p">)</span>
</code></pre></div></div> <p>You just have to define the <code class="language-plaintext highlighter-rouge">forward</code> function, and the <code class="language-plaintext highlighter-rouge">backward</code> function (where gradients are computed) is automatically defined for you using <code class="language-plaintext highlighter-rouge">autograd</code>. You can use any of the Tensor operations in the <code class="language-plaintext highlighter-rouge">forward</code> function.</p> <p>The learnable parameters of a model are returned by <code class="language-plaintext highlighter-rouge">net.parameters()</code></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">params</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="n">net</span><span class="p">.</span><span class="nf">parameters</span><span class="p">())</span>
<span class="nf">print</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">params</span><span class="p">))</span>
<span class="nf">print</span><span class="p">(</span><span class="n">params</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">size</span><span class="p">())</span>  <span class="c1"># conv1's .weight
</span></code></pre></div></div> <ul> <li> <p><strong>NOTE</strong></p> <blockquote> <p><code class="language-plaintext highlighter-rouge">torch.nn</code> only supports mini-batches. The entire <code class="language-plaintext highlighter-rouge">torch.nn</code> package only supports inputs that are a mini-batch of samples, and not a single sample.</p> <p>For example, <code class="language-plaintext highlighter-rouge">nn.Conv2d</code> will take in a 4D Tensor of <code class="language-plaintext highlighter-rouge">nSamples x nChannels x Height x Width</code>.</p> <p>If you have a single sample, just use <code class="language-plaintext highlighter-rouge">input.unsqueeze(0)</code> to add a fake batch dimension.</p> </blockquote> </li> <li> <p><strong>RECAP</strong></p> <blockquote> <p><strong><code class="language-plaintext highlighter-rouge">torch.Tensor</code> - A <em>multi-dimensional array</em> with support for autograd operations like <code class="language-plaintext highlighter-rouge">backward()</code>. Also <em>holds the gradient</em> w.r.t. the tensor.</strong></p> <p><strong><code class="language-plaintext highlighter-rouge">nn.Module</code> - Neural network module. <em>Convenient way of encapsulating parameters</em>, with helpers for moving them to GPU, exporting, loading, etc.</strong></p> <p><strong><code class="language-plaintext highlighter-rouge">nn.Parameter</code> - A kind of Tensor, that is <em>automatically registered as a parameter when assigned as an attribute to a</em> <code class="language-plaintext highlighter-rouge">Module</code>.</strong></p> <p><strong><code class="language-plaintext highlighter-rouge">autograd.Function</code> - Implements <em>forward and backward definitions of an autograd operation</em>. Every <code class="language-plaintext highlighter-rouge">Tensor</code> operation creates at least a single <code class="language-plaintext highlighter-rouge">Function</code> node that connects to functions that created a <code class="language-plaintext highlighter-rouge">Tensor</code> and <em>encodes its history</em>.</strong></p> </blockquote> </li> </ul> <h3 id="loss-function">Loss Function</h3> <p>A loss function takes the (output, target) pair of inputs, and computes a value that estimates how far away the output is from the target.</p> <p>There are several different <a href="https://pytorch.org/docs/nn.html#loss-functions" rel="external nofollow noopener" target="_blank">loss functions</a> under the nn package . A simple loss is: <code class="language-plaintext highlighter-rouge">nn.MSELoss</code> which computes the mean-squared error between the output and the target.</p> <p>For example:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">output</span> <span class="o">=</span> <span class="nf">net</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>  <span class="c1"># a dummy target, for example
</span><span class="n">target</span> <span class="o">=</span> <span class="n">target</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># make it the same shape as output
</span><span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">MSELoss</span><span class="p">()</span>

<span class="n">loss</span> <span class="o">=</span> <span class="nf">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
</code></pre></div></div> <p>Now, if you follow <code class="language-plaintext highlighter-rouge">loss</code> in the backward direction, using its <code class="language-plaintext highlighter-rouge">.grad_fn</code> attribute, you will see a graph of computations that looks like this:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">input</span> <span class="o">-&gt;</span> <span class="n">conv2d</span> <span class="o">-&gt;</span> <span class="n">relu</span> <span class="o">-&gt;</span> <span class="n">maxpool2d</span> <span class="o">-&gt;</span> <span class="n">conv2d</span> <span class="o">-&gt;</span> <span class="n">relu</span> <span class="o">-&gt;</span> <span class="n">maxpool2d</span>
      <span class="o">-&gt;</span> <span class="n">flatten</span> <span class="o">-&gt;</span> <span class="n">linear</span> <span class="o">-&gt;</span> <span class="n">relu</span> <span class="o">-&gt;</span> <span class="n">linear</span> <span class="o">-&gt;</span> <span class="n">relu</span> <span class="o">-&gt;</span> <span class="n">linear</span>
      <span class="o">-&gt;</span> <span class="n">MSELoss</span>
      <span class="o">-&gt;</span> <span class="n">loss</span>
</code></pre></div></div> <p>So, when we call <code class="language-plaintext highlighter-rouge">loss.backward()</code>, the whole graph is differentiated w.r.t. the neural net parameters, and all Tensors in the graph that have <code class="language-plaintext highlighter-rouge">requires_grad=True</code> will have their <code class="language-plaintext highlighter-rouge">.grad</code> Tensor accumulated with the gradient.</p> <h3 id="backprop">Backprop</h3> <p>To backpropagate the error all we have to do is to <code class="language-plaintext highlighter-rouge">loss.backward()</code>. You need to clear the existing gradients though, else gradients will be accumulated to existing gradients.</p> <p>Now we shall call <code class="language-plaintext highlighter-rouge">loss.backward()</code>, and have a look at conv1’s bias gradients before and after the backward.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">net</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>     <span class="c1"># zeroes the gradient buffers of all parameters
</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">conv1.bias.grad before backward</span><span class="sh">'</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">net</span><span class="p">.</span><span class="n">conv1</span><span class="p">.</span><span class="n">bias</span><span class="p">.</span><span class="n">grad</span><span class="p">)</span>

<span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">conv1.bias.grad after backward</span><span class="sh">'</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">net</span><span class="p">.</span><span class="n">conv1</span><span class="p">.</span><span class="n">bias</span><span class="p">.</span><span class="n">grad</span><span class="p">)</span>
</code></pre></div></div> <h3 id="update-the-weights">Update the weights</h3> <p>The simplest update rule used in practice is the Stochastic Gradient Descent (SGD):</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">weight</span> <span class="o">=</span> <span class="n">weight</span> <span class="o">-</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">gradient</span>
</code></pre></div></div> <p>We can implement this using simple Python code:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">net</span><span class="p">.</span><span class="nf">parameters</span><span class="p">():</span>
    <span class="n">f</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nf">sub_</span><span class="p">(</span><span class="n">f</span><span class="p">.</span><span class="n">grad</span><span class="p">.</span><span class="n">data</span> <span class="o">*</span> <span class="n">learning_rate</span><span class="p">)</span>
</code></pre></div></div> <p>However, as you use neural networks, you want to use various different update rules such as SGD, Nesterov-SGD, Adam, RMSProp, etc. To enable this, we built a small package: <code class="language-plaintext highlighter-rouge">torch.optim</code> that implements all these methods. Using it is very simple:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch.optim</span> <span class="k">as</span> <span class="n">optim</span>

<span class="c1"># create your optimizer
</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="p">.</span><span class="nc">SGD</span><span class="p">(</span><span class="n">net</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>

<span class="c1"># in your training loop:
</span><span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>   <span class="c1"># zero the gradient buffers
</span><span class="n">output</span> <span class="o">=</span> <span class="nf">net</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="nf">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
<span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>    <span class="c1"># Does the update
</span></code></pre></div></div> <h2 id="training-a-classifier">Training a Classifier</h2> <p>This is it. You have seen how to define neural networks, compute loss and make updates to the weights of the network.</p> <p>Now you might be thinking,</p> <h3 id="what-about-data">What about Data?</h3> <p>Generally, when you have to deal with image, text, audio or video data, you can use standard python packages that load data into a numpy array. Then you can convert this array into a <code class="language-plaintext highlighter-rouge">torch.*Tensor</code>.</p> <ul> <li>For images, packages such as <strong>Pillow, OpenCV are useful</strong> </li> <li>For audio, packages such as <strong>scipy and librosa</strong> </li> <li>For text, either raw Python or Cython based loading, or <strong>NLTK and SpaCy</strong> are useful</li> </ul> <p>Specifically for vision, we have created a package called <code class="language-plaintext highlighter-rouge">torchvision</code>, that has data loaders for common datasets such as ImageNet, CIFAR10, MNIST, etc. and data transformers for images, viz., <code class="language-plaintext highlighter-rouge">torchvision.datasets</code> and <code class="language-plaintext highlighter-rouge">torch.utils.data.DataLoader</code>.</p> <p>This provides a huge convenience and avoids writing boilerplate code.</p> <p>For this tutorial, we will use the CIFAR10 dataset. It has the classes: ‘airplane’, ‘automobile’, ‘bird’, ‘cat’, ‘deer’, ‘dog’, ‘frog’, ‘horse’, ‘ship’, ‘truck’. The images in CIFAR-10 are of size 3x32x32, i.e. 3-channel color images of 32x32 pixels in size.</p> <h3 id="training-an-image-classifier">Training an image classifier</h3> <p>We will do the following steps in order:</p> <ol> <li>Load and normalize the CIFAR10 training and test datasets using <code class="language-plaintext highlighter-rouge">torchvision</code> </li> <li>Define a Convolutional Neural Network</li> <li>Define a loss function</li> <li>Train the network on the training data</li> <li>Test the network on the test data</li> </ol> <p><strong>Load and Normalize CIFAR10</strong></p> <p>Using <code class="language-plaintext highlighter-rouge">torchvision</code>, it’s extremely easy to load CIFAR10.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torchvision</span>
<span class="kn">import</span> <span class="n">torchvision.transforms</span> <span class="k">as</span> <span class="n">transforms</span>
</code></pre></div></div> <p>The output of torchvision datasets are PILImage images of range [0, 1]. We transform them to Tensors of normalized range [-1, 1].</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="p">.</span><span class="nc">Compose</span><span class="p">(</span>
    <span class="p">[</span><span class="n">transforms</span><span class="p">.</span><span class="nc">ToTensor</span><span class="p">(),</span>
     <span class="n">transforms</span><span class="p">.</span><span class="nc">Normalize</span><span class="p">((</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">),</span> <span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">))])</span>

<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">4</span>

<span class="n">trainset</span> <span class="o">=</span> <span class="n">torchvision</span><span class="p">.</span><span class="n">datasets</span><span class="p">.</span><span class="nc">CIFAR10</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="sh">'</span><span class="s">./data</span><span class="sh">'</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                                        <span class="n">download</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">)</span>
<span class="n">trainloader</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nc">DataLoader</span><span class="p">(</span><span class="n">trainset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                                          <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="n">testset</span> <span class="o">=</span> <span class="n">torchvision</span><span class="p">.</span><span class="n">datasets</span><span class="p">.</span><span class="nc">CIFAR10</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="sh">'</span><span class="s">./data</span><span class="sh">'</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
                                       <span class="n">download</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">)</span>
<span class="n">testloader</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nc">DataLoader</span><span class="p">(</span><span class="n">testset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                                         <span class="n">shuffle</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="n">classes</span> <span class="o">=</span> <span class="p">(</span><span class="sh">'</span><span class="s">plane</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">car</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">bird</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">cat</span><span class="sh">'</span><span class="p">,</span>
           <span class="sh">'</span><span class="s">deer</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">dog</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">frog</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">horse</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">ship</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">truck</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div> <p>Let us show some of the training images, for fun.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="c1"># functions to show an image
</span>
<span class="k">def</span> <span class="nf">imshow</span><span class="p">(</span><span class="n">img</span><span class="p">):</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">img</span> <span class="o">/</span> <span class="mi">2</span> <span class="o">+</span> <span class="mf">0.5</span>     <span class="c1"># unnormalize
</span>    <span class="n">npimg</span> <span class="o">=</span> <span class="n">img</span><span class="p">.</span><span class="nf">numpy</span><span class="p">()</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">imshow</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="n">npimg</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">)))</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>

<span class="c1"># get some random training images
</span><span class="n">dataiter</span> <span class="o">=</span> <span class="nf">iter</span><span class="p">(</span><span class="n">trainloader</span><span class="p">)</span>
<span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="nf">next</span><span class="p">(</span><span class="n">dataiter</span><span class="p">)</span>

<span class="c1"># show images
</span><span class="nf">imshow</span><span class="p">(</span><span class="n">torchvision</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="nf">make_grid</span><span class="p">(</span><span class="n">images</span><span class="p">))</span>
<span class="c1"># print labels
</span><span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s"> </span><span class="sh">'</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="si">{</span><span class="n">classes</span><span class="p">[</span><span class="n">labels</span><span class="p">[</span><span class="n">j</span><span class="p">]]</span><span class="si">:</span><span class="mi">5</span><span class="n">s</span><span class="si">}</span><span class="sh">'</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)))</span>
</code></pre></div></div> <p><strong>Define a Convolutional Neural Network</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="n">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span>

<span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">pool</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">MaxPool2d</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">16</span> <span class="o">*</span> <span class="mi">5</span> <span class="o">*</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">120</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">120</span><span class="p">,</span> <span class="mi">84</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">fc3</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">84</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">pool</span><span class="p">(</span><span class="n">F</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">pool</span><span class="p">(</span><span class="n">F</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">flatten</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="c1"># flatten all dimensions except batch
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">fc3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

<span class="n">net</span> <span class="o">=</span> <span class="nc">Net</span><span class="p">()</span>
</code></pre></div></div> <p><strong>Define a Loss function and optimizer</strong></p> <p>Let’s use a Classification Cross-Entropy loss and SGD with momentum.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch.optim</span> <span class="k">as</span> <span class="n">optim</span>

<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">CrossEntropyLoss</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="p">.</span><span class="nc">SGD</span><span class="p">(</span><span class="n">net</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
</code></pre></div></div> <p><strong>Train the network</strong></p> <p>This is when things start to get interesting. We simply have to loop over our data iterator, and feed the inputs to the network and optimize.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span>  <span class="c1"># loop over the dataset multiple times
</span>
    <span class="n">running_loss</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">data</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">trainloader</span><span class="p">,</span> <span class="mi">0</span><span class="p">):</span>
        <span class="c1"># get the inputs; data is a list of [inputs, labels]
</span>        <span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">data</span>

        <span class="c1"># zero the parameter gradients
</span>        <span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>

        <span class="c1"># forward + backward + optimize
</span>        <span class="n">outputs</span> <span class="o">=</span> <span class="nf">net</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="nf">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
        <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>

        <span class="c1"># print statistics
</span>        <span class="n">running_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="p">.</span><span class="nf">item</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">2000</span> <span class="o">==</span> <span class="mi">1999</span><span class="p">:</span>    <span class="c1"># print every 2000 mini-batches
</span>            <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">[</span><span class="si">{</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="si">}</span><span class="s">, </span><span class="si">{</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="si">:</span><span class="mi">5</span><span class="n">d</span><span class="si">}</span><span class="s">] loss: </span><span class="si">{</span><span class="n">running_loss</span> <span class="o">/</span> <span class="mi">2000</span><span class="si">:</span><span class="p">.</span><span class="mi">3</span><span class="n">f</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
            <span class="n">running_loss</span> <span class="o">=</span> <span class="mf">0.0</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Finished Training</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div> <p>Save our trained model:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">PATH</span> <span class="o">=</span> <span class="sh">'</span><span class="s">./cifar_net.pth</span><span class="sh">'</span>
<span class="n">torch</span><span class="p">.</span><span class="nf">save</span><span class="p">(</span><span class="n">net</span><span class="p">.</span><span class="nf">state_dict</span><span class="p">(),</span> <span class="n">PATH</span><span class="p">)</span>
</code></pre></div></div> <p><strong>Test the network on the test data</strong></p> <p>We have trained the network for 2 passes over the training dataset. But we need to check if the network has learnt anything at all.</p> <p>We will check this by predicting the class label that the neural network outputs, and checking it against the ground-truth. If the prediction is correct, we add the sample to the list of correct predictions.</p> <p>Okay, first step. Let us display an image from the test set to get familiar.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">dataiter</span> <span class="o">=</span> <span class="nf">iter</span><span class="p">(</span><span class="n">testloader</span><span class="p">)</span>
<span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="nf">next</span><span class="p">(</span><span class="n">dataiter</span><span class="p">)</span>

<span class="c1"># print images
</span><span class="nf">imshow</span><span class="p">(</span><span class="n">torchvision</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="nf">make_grid</span><span class="p">(</span><span class="n">images</span><span class="p">))</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">GroundTruth: </span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s"> </span><span class="sh">'</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="si">{</span><span class="n">classes</span><span class="p">[</span><span class="n">labels</span><span class="p">[</span><span class="n">j</span><span class="p">]]</span><span class="si">:</span><span class="mi">5</span><span class="n">s</span><span class="si">}</span><span class="sh">'</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">4</span><span class="p">)))</span>
</code></pre></div></div> <p>Next, let’s load back in our saved model (note: saving and re-loading the model wasn’t necessary here, we only did it to illustrate how to do so):</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Load trained model
</span><span class="n">net</span> <span class="o">=</span> <span class="nc">Net</span><span class="p">()</span>
<span class="n">net</span><span class="p">.</span><span class="nf">load_state_dict</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="n">PATH</span><span class="p">))</span>

<span class="c1"># Generate outputs
</span><span class="n">outputs</span> <span class="o">=</span> <span class="nf">net</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>

<span class="c1"># Show predictions
</span><span class="n">_</span><span class="p">,</span> <span class="n">predicted</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">max</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Predicted: </span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s"> </span><span class="sh">'</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="si">{</span><span class="n">classes</span><span class="p">[</span><span class="n">predicted</span><span class="p">[</span><span class="n">j</span><span class="p">]]</span><span class="si">:</span><span class="mi">5</span><span class="n">s</span><span class="si">}</span><span class="sh">'</span>
                              <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">4</span><span class="p">)))</span>
</code></pre></div></div> <p>Let us look at how the network performs on the whole dataset.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">correct</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">total</span> <span class="o">=</span> <span class="mi">0</span>
<span class="c1"># since we're not training, we don't need to calculate the gradients for our outputs
</span><span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">testloader</span><span class="p">:</span>
        <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">data</span>
        <span class="c1"># calculate outputs by running images through the network
</span>        <span class="n">outputs</span> <span class="o">=</span> <span class="nf">net</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
        <span class="c1"># the class with the highest energy is what we choose as prediction
</span>        <span class="n">_</span><span class="p">,</span> <span class="n">predicted</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">max</span><span class="p">(</span><span class="n">outputs</span><span class="p">.</span><span class="n">data</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">total</span> <span class="o">+=</span> <span class="n">labels</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">correct</span> <span class="o">+=</span> <span class="p">(</span><span class="n">predicted</span> <span class="o">==</span> <span class="n">labels</span><span class="p">).</span><span class="nf">sum</span><span class="p">().</span><span class="nf">item</span><span class="p">()</span>

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">Accuracy of the network on the 10000 test images: </span><span class="si">{</span><span class="mi">100</span> <span class="o">*</span> <span class="n">correct</span> <span class="o">//</span> <span class="n">total</span><span class="si">}</span><span class="s"> %</span><span class="sh">'</span><span class="p">)</span> <span class="c1"># Accuracy = 54%
</span></code></pre></div></div> <p>That looks way better than chance, which is 10% accuracy (randomly picking a class out of 10 classes). Seems like the network learnt something.</p> <p>Now, look out what are the classes that performed well, and the classes that did not perform well:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># prepare to count predictions for each class
</span><span class="n">correct_pred</span> <span class="o">=</span> <span class="p">{</span><span class="n">classname</span><span class="p">:</span> <span class="mi">0</span> <span class="k">for</span> <span class="n">classname</span> <span class="ow">in</span> <span class="n">classes</span><span class="p">}</span>
<span class="n">total_pred</span> <span class="o">=</span> <span class="p">{</span><span class="n">classname</span><span class="p">:</span> <span class="mi">0</span> <span class="k">for</span> <span class="n">classname</span> <span class="ow">in</span> <span class="n">classes</span><span class="p">}</span>

<span class="c1"># again no gradients needed
</span><span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">testloader</span><span class="p">:</span>
        <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">data</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="nf">net</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">predictions</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">max</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="c1"># collect the correct predictions for each class
</span>        <span class="k">for</span> <span class="n">label</span><span class="p">,</span> <span class="n">prediction</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">predictions</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">label</span> <span class="o">==</span> <span class="n">prediction</span><span class="p">:</span>
                <span class="n">correct_pred</span><span class="p">[</span><span class="n">classes</span><span class="p">[</span><span class="n">label</span><span class="p">]]</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="n">total_pred</span><span class="p">[</span><span class="n">classes</span><span class="p">[</span><span class="n">label</span><span class="p">]]</span> <span class="o">+=</span> <span class="mi">1</span>

<span class="c1"># print accuracy for each class
</span><span class="k">for</span> <span class="n">classname</span><span class="p">,</span> <span class="n">correct_count</span> <span class="ow">in</span> <span class="n">correct_pred</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
    <span class="n">accuracy</span> <span class="o">=</span> <span class="mi">100</span> <span class="o">*</span> <span class="nf">float</span><span class="p">(</span><span class="n">correct_count</span><span class="p">)</span> <span class="o">/</span> <span class="n">total_pred</span><span class="p">[</span><span class="n">classname</span><span class="p">]</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">Accuracy for class: </span><span class="si">{</span><span class="n">classname</span><span class="si">:</span><span class="mi">5</span><span class="n">s</span><span class="si">}</span><span class="s"> is </span><span class="si">{</span><span class="n">accuracy</span><span class="si">:</span><span class="p">.</span><span class="mi">1</span><span class="n">f</span><span class="si">}</span><span class="s"> %</span><span class="sh">'</span><span class="p">)</span>

<span class="o">---------------</span>
<span class="sh">"""</span><span class="s">
Accuracy for class: plane is 51.9 %
Accuracy for class: car   is 68.3 %
Accuracy for class: bird  is 39.3 %
Accuracy for class: cat   is 25.1 %
Accuracy for class: deer  is 62.1 %
Accuracy for class: dog   is 39.1 %
Accuracy for class: frog  is 67.9 %
Accuracy for class: horse is 65.6 %
Accuracy for class: ship  is 72.3 %
Accuracy for class: truck is 56.8 %
</span><span class="sh">"""</span>
</code></pre></div></div> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/myblog/2023/build-ds-project/">Get lost when trying to build a Data Science Project</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/myblog/2023/error_exception_python/">Error and Exception in Python</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/myblog/2023/performance-measures/">Performance Metrics in Machine Learning</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/myblog/2023/review-machine-learning/">Basic Concepts of Machine Learning</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/myblog/2023/machine-learning-deep-learning/">Difference Between Machine Learning and Deep Learning</a> </li> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 Duc Minh Quang Nguyen. # Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. # Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. # Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?07b8786bab9b4abe90d10e61f7d12ff7" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>