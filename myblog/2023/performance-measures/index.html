<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Performance Metrics in Machine Learning | Duc Minh Quang Nguyen</title> <meta name="author" content="Duc Minh Quang Nguyen"> <meta name="description" content="# A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://nminhquang380.github.io/myblog/2023/performance-measures/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Duc Minh Quang </span>Nguyen</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Portfolio</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">Resume</a> </li> <li class="nav-item "> <a class="nav-link" href="/bucket_list/">My Bucket List</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Performance Metrics in Machine Learning</h1> <p class="post-meta">August 1, 2023</p> <p class="post-tags"> <i class="fa-solid fa-calendar fa-sm"></i> 2023   ·   <i class="fa-solid fa-tag fa-sm"></i> machinelearning   </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Performance metrics in machine learning are essential for assessing the effectiveness and reliability of models. They’re a key element of every machine learning pipeline, allowing developers to fine-tune their algorithms and drive improvements.</p> <p>The metrics can be broadly categorized into two main types: regression and classification metrics.</p> <h2 id="top-regression-metrics">Top regression metrics</h2> <h3 id="mean-absolute-error-mae">Mean Absolute Error (MAE)</h3> <p>It measures the average magnitude of errors between predicted and actual values without considering their direction. MAE is especially useful in applications that aim to minimize the average error and is less sensitive to outliers than other metrics like Mean Squared Error (MSE).</p> \[MAE = \frac{1}{n}\sum_i^n|y_i - \hat{y_i}|\] <ul> <li> <p><strong>What it shows</strong>: MAE measures the average magnitude of errors in the predictions made by the model (without considering their direction).</p> </li> <li> <p><strong>When to use</strong>: Use MAE when you want a simple, interpretable metric to evaluate the performance of your regression model.</p> </li> <li> <p><strong>When to avoid</strong>: Avoid using MAE to emphasize the impact of larger errors, as it does not penalize them heavily.</p> </li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>

<span class="n">act_vals</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">8</span><span class="p">])</span>
<span class="n">pred_vals</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([</span><span class="mf">2.5</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">,</span> <span class="mf">6.5</span><span class="p">,</span> <span class="mf">7.5</span><span class="p">])</span>

<span class="k">def</span> <span class="nf">mean_absolute_error</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">):</span>
    <span class="n">abs_diff</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">abs</span><span class="p">(</span><span class="n">y_true</span> <span class="o">-</span> <span class="n">y_pred</span><span class="p">)</span>
    <span class="n">mae</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">abs_diff</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">mae</span>

<span class="n">mae</span> <span class="o">=</span> <span class="nf">mean_absolute_error</span><span class="p">(</span><span class="n">act_vals</span><span class="p">,</span> <span class="n">pred_vals</span><span class="p">)</span>
</code></pre></div></div> <h3 id="mean-squared-error-mse">Mean Squared Error (MSE)</h3> <p>It measures the average squared difference between the predicted and actual values, thus emphasizing larger errors. MSE is particularly useful in applications where the goal is to minimize the impact of outliers or when the error distribution is assumed to be Gaussian.</p> \[MSE = \frac{1}{n} \sum_{i=1}^n (y_i - \hat{y_i})^2\] <ul> <li> <p>What it shows: MSE measures the average squared difference between the actual and predicted values, penalizing larger errors more heavily than smaller ones.</p> </li> <li> <p>When to use: Use MSE when you want to place a higher emphasis on larger errors.</p> </li> <li> <p>When not to use: Avoid using MSE if you need an easily interpretable metric or if <strong>your dataset has a lot of outliers, as it can be sensitive to them.</strong> ```python import torch</p> </li> </ul> <p>act_vals = torch.tensor([2, 4, 6, 8]) pred_vals = torch.tensor([2.5, 4.5, 6.5, 8.5])</p> <p>def mean_squared_error(y_true, y_pred): diff = (y_true - y_pred) ** 2 mse = torch.mean(diff) return diff</p> <p>mse = mean_squared_error(act_vals, pred_vals) print(f”mse: {mse:.2f}”)</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>### Root Mean Squared Error (RMSE)
Root Mean Squared Error (RMSE) has the same unit as the target variable, making it more interpretable and easier to relate to the problem context than MSE.

$$RMSE = \sqrt{MSE}$$

- **When to use**:
Use RMSE to penalize larger errors and obtain a metric with the same unit as the target variable.

- **When not to use**:
Avoid using RMSE if you need an interpretable metric or if your dataset has a lot of outliers.

### R-Squared
R Squared (R^2), also known as the coefficient of determination, measures the proportion of the total variation in the target variable explained by the model's predictions.

R^2 ranges from 0 to 1, with higher values indicating a better model fit.

- **What it shows**:
R-squared measures the proportion of the variance in the dependent variable that the model's independent variables can explain.

- **When to use**:
Use R-squared when you want to understand how well your model is explaining the variation in the target variable compared to a simple average.

- **When not to use**:
Avoid using it if your model has a large number of independent variables or if it is sensitive to outliers.

```python
import torch

# Create tensors for actual and predicted values
actual_values = torch.tensor([2.0, 4.0, 6.0, 8.0])
predicted_values = torch.tensor([2.5, 3.5, 6.5, 7.5])

def r_squared_error(y_true, y_pred):
    # Calculate the mean of the actual values
    y_mean = torch.mean(y_true)
    
    # Calculate the sum of squares (numerator)
    residual_sum_of_squares = torch.sum((y_true - y_pred) ** 2)
    
    # Calculate the total sum of squares (denominator)
    total_sum_of_squares = torch.sum((y_true - y_mean) ** 2)
    
    # Calculate R² using the formula
    r_squared = 1 - (residual_sum_of_squares / total_sum_of_squares)
    
    return r_squared

# Calculate R²
r_squared = r_squared_error(actual_values, predicted_values)
print(f"R Squared Error: {r_squared:.2f}")
</code></pre></div></div> <h2 id="top-classification-metrics">Top classification metrics</h2> <h3 id="accuraccy">Accuraccy</h3> <p>Accuracy is a fundamental evaluation metric for assessing the overall performance of a classification model.</p> \[Accuraccy = \frac{TP + TN}{TP + FP + TN + FN}\] <p><strong>What it shows</strong>: Accuracy measures the proportion of correct predictions made by the model out of all predictions.</p> <p><strong>When to use</strong>: Accuracy is useful when the class distribution is balanced, and false positives and negatives have equal importance.</p> <p><strong>When not to use</strong>: If the dataset is imbalanced or the cost of false positives and negatives differs, accuracy may not be an appropriate metric.</p> <h3 id="confusion-matrix">Confusion Matrix</h3> <p>A confusion matrix, also known as an error matrix, is a tool used to evaluate the performance of classification models in machine learning and statistics. It presents a summary of the predictions made by a classifier compared to the actual class labels, allowing for a detailed analysis of the classifier’s performance across different classes.</p> <p>It helps identify misclassification patterns and calculate various evaluation metrics such as precision, recall, F1-score, and accuracy. By analyzing the confusion matrix, you can diagnose the model’s strengths and weaknesses and improve its performance.</p> <p>TP: True Positives - The number of patients with the disease correctly predicted as “yes.”</p> <p>TN: True Negatives - The number of patients without the disease was correctly predicted as “no.”</p> <p>FP: False Positives - The number of patients who don’t have the disease but were incorrectly predicted as “yes.”</p> <p>FN: False Negatives - The number of patients who have the disease but were incorrectly predicted as “no.”</p> <h3 id="precision-and-recall">Precision and Recall</h3> <p>Precision and recall are essential evaluation metrics in machine learning for understanding the trade-off between false positives and false negatives.</p> <p>\(Precision = \frac{TP}{TP + FP}\) \(Recall = \frac{TP}{TP+FN}\)</p> <p>Precision (P) is the proportion of true positive predictions among all positive predictions. It is a measure of how accurate the positive predictions are.</p> <p>Recall (R), also known as sensitivity or true positive rate (TPR), is the proportion of true positive predictions among all actual positive instances. It measures the classifier’s ability to identify positive instances correctly.</p> <p>A high precision means the model has fewer false positives, while a high recall means fewer false negatives. Depending on the specific problem you’re trying to solve, you might prioritize one of these metrics over the other.</p> <p><strong>What they show</strong> Precision measures the proportion of true positive predictions among all positive predictions, while recall measures the proportion of true positive predictions among all actual positive instances.</p> <p><strong>When to use</strong> Precision and recall are useful when the class distribution is imbalanced or when the cost of false positives and false negatives is different.</p> <p><strong>When not to use</strong> Accuracy might be more appropriate if the dataset is balanced and the costs of false positives and negatives are equal.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>

<span class="k">def</span> <span class="nf">precision_recall</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">):</span>
    <span class="k">assert</span> <span class="n">y_true</span><span class="p">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">y_pred</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="sh">"</span><span class="s">Input tensors must have the same shape</span><span class="sh">"</span>
    
    <span class="c1"># Convert predictions to binary (0 or 1) by applying a threshold (0.5 in this case)
</span>    <span class="n">y_pred_binary</span> <span class="o">=</span> <span class="p">(</span><span class="n">y_pred</span> <span class="o">&gt;=</span> <span class="mf">0.5</span><span class="p">).</span><span class="nf">float</span><span class="p">()</span>
    
    <span class="c1"># Calculate True Positives (TP), False Positives (FP), and False Negatives (FN)
</span>    <span class="n">TP</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">y_true</span> <span class="o">*</span> <span class="n">y_pred_binary</span><span class="p">)</span>
    <span class="n">FP</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sum</span><span class="p">((</span><span class="mi">1</span> <span class="o">-</span> <span class="n">y_true</span><span class="p">)</span> <span class="o">*</span> <span class="n">y_pred_binary</span><span class="p">)</span>
    <span class="n">FN</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">y_true</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">y_pred_binary</span><span class="p">))</span>
    
    <span class="c1"># Calculate Precision and Recall
</span>    <span class="n">precision</span> <span class="o">=</span> <span class="n">TP</span> <span class="o">/</span> <span class="p">(</span><span class="n">TP</span> <span class="o">+</span> <span class="n">FP</span><span class="p">)</span>
    <span class="n">recall</span> <span class="o">=</span> <span class="n">TP</span> <span class="o">/</span> <span class="p">(</span><span class="n">TP</span> <span class="o">+</span> <span class="n">FN</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">precision</span><span class="p">,</span> <span class="n">recall</span>

<span class="c1"># Example usage
</span><span class="n">y_true</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">])</span>

<span class="n">precision</span><span class="p">,</span> <span class="n">recall</span> <span class="o">=</span> <span class="nf">precision_recall</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Precision: </span><span class="si">{</span><span class="n">precision</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="s">, Recall: </span><span class="si">{</span><span class="n">recall</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <h3 id="f1-score">F1 Score</h3> <p>The F1-score is the harmonic mean of precision and recall, providing a metric that balances both measures. It is beneficial when dealing with imbalanced datasets, where one class is significantly more frequent than the other. The formula for the F1 score is:</p> \[F1 Score = \frac{2 \times Precision \times Recall}{Precision + Recall}\] <p><strong>What it shows</strong> The F1-score is the harmonic mean of precision and recall, providing a metric that considers false positives and false negatives.</p> <p><strong>When to use</strong> The F1-score is useful when the class distribution is imbalanced or when the cost of false positives and false negatives is different.</p> <p><strong>When not to use</strong> Accuracy might be more appropriate if the dataset is balanced and the costs of false positives and negatives are equal.</p> <h3 id="area-under-the-receiver-operating-characteristic-curve-au-roc">Area Under the Receiver Operating Characteristic Curve (AU-ROC)</h3> <p>The AU-ROC is a popular evaluation metric for binary classification problems. It measures the model’s ability to distinguish between positive and negative classes. The ROC curve plots the true positive rate (recall) against the false positive rate (1 - specificity) at various classification thresholds. The AU-ROC represents the area under the ROC curve, and a higher value indicates better model performance.</p> <p><strong>What it shows</strong> AU-ROC represents the model’s ability to discriminate between positive and negative classes. A higher AU-ROC value indicates better classification performance.</p> <p><strong>When to use</strong> Use AU-ROC to compare the performance of different classification models, especially when the class distribution is imbalanced.</p> <p><strong>When not to use</strong> Accuracy might be more appropriate if the dataset is balanced and the costs of false positives and negatives are equal.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="n">sklearn.metrics</span> <span class="kn">import</span> <span class="n">roc_auc_score</span>

<span class="c1"># Assuming you have the following PyTorch tensors:
# - `y_true`: a 1D tensor containing the true binary labels (0 or 1) for each sample
# - `y_pred`: a 1D tensor containing the predicted probabilities for the positive class
</span>
<span class="c1"># Convert tensors to NumPy arrays
</span><span class="n">y_true_np</span> <span class="o">=</span> <span class="n">y_true</span><span class="p">.</span><span class="nf">detach</span><span class="p">().</span><span class="nf">cpu</span><span class="p">().</span><span class="nf">numpy</span><span class="p">()</span>
<span class="n">y_pred_np</span> <span class="o">=</span> <span class="n">y_pred</span><span class="p">.</span><span class="nf">detach</span><span class="p">().</span><span class="nf">cpu</span><span class="p">().</span><span class="nf">numpy</span><span class="p">()</span>

<span class="c1"># Calculate AUROC score using scikit-learn
</span><span class="n">auroc</span> <span class="o">=</span> <span class="nf">roc_auc_score</span><span class="p">(</span><span class="n">y_true_np</span><span class="p">,</span> <span class="n">y_pred_np</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">AUROC score: </span><span class="si">{</span><span class="n">auroc</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <h2 id="other-important-metrics">Other important metrics</h2> <h3 id="intersection-over-union-iou">Intersection over Union (IoU)</h3> <p>Intersection over Union (IoU) is a popular evaluation metric in object detection and segmentation tasks. It measures the overlap between the predicted bounding box and the ground truth bounding box, providing an understanding of how well the model detects objects in images.</p> <p>A higher IoU value indicates a better model performance, with 1.0 being the perfect score.</p> <h3 id="mean-average-precision-map">Mean Average Precision (MAP)</h3> <p>Mean Average Precision (mAP) is another widely used performance metric in object detection and segmentation tasks. It is the average of the precision values calculated at different recall levels, providing a single value that captures the overall effectiveness of the model.</p> <p><strong>What it shows</strong> Mean Average Precision (mAP) is a metric that computes the average precision (AP) for multiple object classes. It combines precision and recall, considering the presence of false positives and false negatives and their distribution across different confidence thresholds. The mAP score ranges from 0 (worst performance) to 1 (best performance).</p> <p><strong>When to use</strong> Use mAP in object detection and segmentation tasks to evaluate the model’s overall performance across all object classes—when there are multiple object classes, and you want a single metric to assess the model’s performance across all classes.</p> <p><strong>When not to use</strong> Avoid using mAP when you need a detailed analysis of the model’s performance in specific classes, as it averages the performance across all classes. In such cases, analyze class-wise AP instead.</p> <h2 id="final-words">Final words</h2> <ul> <li>Different machine learning tasks require specific evaluation metrics. Regression tasks commonly use metrics such as Mean Squared Error (MSE), Root Mean Squared Error (RMSE), and R² (R-Squared). In contrast, classification tasks use metrics like Accuracy, Confusion Matrix, Precision and Recall, F1-score, and AU-ROC. Object detection and segmentation tasks rely on metrics like Intersection over Union (IoU) and Mean Average Precision (mAP).</li> <li>Choosing the right metric for a given project requires a clear understanding of the project goals and business objectives. Different metrics prioritize different aspects of model performance, and selecting the most relevant metric ensures that the model is optimized to meet the project’s specific needs.</li> <li>Be aware of the strengths and weaknesses of each metric. For example, accuracy is a simple and intuitive metric for classification tasks but can be misleading for imbalanced datasets. Metrics like Precision, Recall, and F1-score may be more appropriate.</li> <li>Consistently use the chosen metric across various models and algorithms to effectively compare their performance. Doing so lets you identify the best-performing model that aligns with your project goals and business objectives.</li> </ul> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/myblog/2023/build-ds-project/">Get lost when trying to build a Data Science Project</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/myblog/2023/intro_pytorch/">Introduction to PyTorch</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/myblog/2023/error_exception_python/">Error and Exception in Python</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/myblog/2023/review-machine-learning/">Basic Concepts of Machine Learning</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/myblog/2023/eda/">Exploratory Data Analysis</a> </li> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2023 Duc Minh Quang Nguyen. # Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. # Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. # Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?07b8786bab9b4abe90d10e61f7d12ff7" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>