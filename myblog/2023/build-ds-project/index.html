<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Get lost when trying to build a Data Science Project | Duc Minh Quang Nguyen</title> <meta name="author" content="Duc Minh Quang Nguyen"> <meta name="description" content="# A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://nminhquang380.github.io/myblog/2023/build-ds-project/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Duc Minh Quang </span>Nguyen</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Portfolio</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">Resume</a> </li> <li class="nav-item "> <a class="nav-link" href="/bucket_list/">My Bucket List</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Get lost when trying to build a Data Science Project</h1> <p class="post-meta">September 7, 2023</p> <p class="post-tags"> <i class="fa-solid fa-calendar fa-sm"></i> 2023   ·   <i class="fa-solid fa-tag fa-sm"></i> datascience   </p> </header> <article class="post-content"> <div id="markdown-content"> <h3 id="disclaimer">Disclaimer</h3> <p>Currently I am lost in Data Science where there are a lot of concepts, technique and skills I should learn. Additionally, next year I will graduate so I should prepare something to get a job, especially in Australia which have a roughly competitive work market. Thus I try to build some DS projects, however, I got some issues, and this article helps me a lot, let me share it!</p> <hr> <h2 id="how-to-build-a-data-science-project-from-scratch">How to build a data science project from scratch</h2> <p>There are many online courses about data science and machine learning that will guide you through a theory and provide you with some code examples and an analysis of very clean data.</p> <p>However, in order to start practising data science, it is better if you challenge a real-life problem. Digging into the data in order to find deeper insights. Carrying out feature engineering using additional sources of data and building stand-alone machine learning pipelines.</p> <p>This blog post will guide you through the main steps of building a data science project from scratch. It is based on a real-life problem — what are the main drivers of rental prices in Berlin? It will provide an analysis of this situation. It will also highlight the common mistake beginners tend to make when it comes to machine learning.</p> <p>These are the steps that will be discussed in detail:</p> <ul> <li>finding a topic</li> <li>extracting data from the web and cleaning it</li> <li>gaining deeper insights</li> <li>engineering of features using external APIs</li> <li>common mistakes while carrying out machine learning</li> <li>feature importance: finding the drivers of rental prices</li> <li>building machine learning models.</li> </ul> <h3 id="finding-a-topic">Finding a topic</h3> <p>There are many problems that can be solved by analyzing data, but it is always better to find a problem that you are interested in and that will motivate you. While searching for a topic, you should definitely concentrate on your preferences and interests.</p> <p>For instance, if you are interested in healthcare systems, there are many angles from which you could challenge the data provided on that topic. “Exploring the ChestXray14 dataset: problems” is an example of how to question the quality of medical data. Another example — if you are interested in music, you could try to predict the genre of the song from its audio.</p> <p>However, I suggest not only to concentrate on your interests but also to listen to what people around you are talking about. What bothers them? What are they complaining about? This can be another good source of ideas for a data science project. In those cases where people are still complaining about it, this may mean that the problem wasn’t solved properly the first time around. Thus, if you challenge it with data, you could provide an even better solution and have an impact in how this topic is perceived.</p> <p>This may all sound a bit too abstract, so lets find out how I came up with the idea to analyze Berlin rental prices.</p> <blockquote> <p>“If I had known that the rental prices were so high here, I would have negotiated for a higher salary.”</p> </blockquote> <p>This is just one of the things I heard from people who had recently moved to Berlin for work. Most newcomers complained that they hadn’t imagined Berlin to be so expensive, and that there were no statistics about possible price ranges of the apartment. If they had known this it beforehand, they could have asked for a higher salary during the job application process or could have considered other options.</p> <p>I googled, checked several rental apartment websites, and asked several people, but could not find any plausible statistics or visualizations of the current market prices. And this was how I came up with the idea of this analysis.</p> <p>I wanted to gather the data, build an interactive dashboard where you could select different options such as a 40m2 apartment situated in Berlin Mitte with a balcony and equipped kitchen, and it would show you the price ranges. This, alone, would help people understand apartment prices in Berlin. Also, by applying machine learning, I would be able to identify the drivers of the rental prices and practice with different machine learning algorithms.</p> <h3 id="extracting-data-from-the-web-and-cleaning-it">Extracting data from the web and cleaning it</h3> <h4 id="getting-the-data">Getting the data</h4> <p>Now that you have an idea about your data science project, you can start looking for the data. There are tons of amazing data repositories, such as Kaggle, UCI ML Repository or dataset search engines, and websites containing academic papers with datasets. Alternatively, you could use web scraping.</p> <p>But be cautious — old data is everywhere. When I was searching for the information about the rental prices in Berlin, I found many visualizations but they were old, or without any year specified.</p> <p>For some statistics, they even had a note saying that this price would only be for a 2 room apartment of 50 m2 without furniture. But what if I am searching for a smaller apartment with a furnished kitchen?</p> <p>As I could find only old data, I decided to <a href="https://hackernoon.com/web-scraping-tutorial-with-python-tips-and-tricks-db070e70e071" rel="external nofollow noopener" target="_blank">web scrape</a> the websites that offered rental apartments. Web scraping is a technique used to extract data from websites through an automated process.</p> <p>Here are the main findings:</p> <ul> <li>Before scraping, check if there is a public API available</li> <li>Be kind! Don’t overload the website by sending hundreds of requests per second</li> <li>Save the date when the extraction took place. It will be explained why this is important.</li> </ul> <h4 id="data-cleaning">Data cleaning</h4> <p>Once you starting getting the data, it is very important to have a look at it as early as possible in order to find any possible issues.</p> <p>While web scraping rental data, I included some small checks such as the number of missing values for all features. Web-masters could change the HTML of the website, which would result in my program not getting the data anymore.</p> <p>Once I had ensured that all technical aspects of web scraping were covered, I thought the data would almost be ideal. However, I ended up cleaning the data for around a week because of not so obvious duplicates.</p> <p>Once you starting getting the data, it is very important to have a look at it as early as possible in order to find any possible issues. For instance, if you web scrape, you could have missed some important fields. If you use a comma separator while saving data into a file, and one of the fields also contains commas, you can end up having files which are not separated very well.</p> <p>There were several sources of duplicates:</p> <ul> <li>Duplicated apartments because they had been online for a while</li> <li>Agencies had input errors, for example the rental price or the storey of the apartment. They would correct them after a while, or would publish a completely new ad with corrected values and additional description modifications</li> <li>Some prices were changed (increased and decreased) after a month for the same apartment</li> </ul> <p>While the duplicates from the first case were easy to identify by their ID, the duplicates from the second case were very complicated. The reason is that an agency could slightly change a description, modify the wrong price, and publish it as a new ad so that the ID would also be new.</p> <p>I had to come up with many logic-based rules to filter out the old versions of the ads. Once I was able to identify that these apartments would be the actual duplicates but with slight modifications, I could sort them by the extraction date, taking the latest one as the most recent.</p> <p>Additionally, some agencies would increase or decrease the price for the same apartment after a month. I was told that if nobody wanted this apartment, the price would decrease. Conversely, I was told that, if there were so many requests for it, that the agencies increased the price. These sounds like good explanations.</p> <h4 id="gaining-deeper-insights">Gaining deeper insights</h4> <p>Now that we have everything ready, we can start analyzing the data. I know data scientists love <code class="language-plaintext highlighter-rouge">seaborn</code> and <code class="language-plaintext highlighter-rouge">ggplot2</code>, as well as many static visualizations from which they can derive some insights.</p> <p>However, interactive dashboards can help you and other stakeholders to find useful insights. There are many amazing easy-to-use tools for that, such as <code class="language-plaintext highlighter-rouge">Tableau</code> and <code class="language-plaintext highlighter-rouge">Microstrategy</code>.</p> <p>It took me less than 30 minutes to create an interactive dashboard where one can select all the important components and see how the price would change.</p> <p>A fairly simple dashboard could already provide insights into the prices in Berlin for newcomers and could be a good user driver for a rental apartment website.</p> <p>Already from this data visualization you can see that the price distribution of 2.5 rooms falls into the distribution of 2 room apartment. The reason for this is that most of the 2.5 room apartments aren’t situated in the center of the city which, of course, reduces the price.</p> <p>This data was gathered in winter 2017/18 and it will also get outdated. However, my point is that the rental websites could frequently update their statistics and visualizations to provide more transparency to this question.</p> <h4 id="engineering-of-features-using-external-apis">Engineering of features using external APIs</h4> <p>Visualization helps you to identify important attributes, or “features,” that could be used by these machine learning algorithms. If the features you use are very uninformative, any algorithm will produce bad predictions. With very strong features, even a very simple algorithm can produce pretty decent results.</p> <p>In the rental price project, price is a continuous variable, so it is a typical regression problem. Taking all extracted information, I collected the following features in order to be able to predict a rental price.</p> <p>However, there was one feature that was problematic, namely the address. There were 6.6K apartments and around 4.4K unique addresses of different granularity. There were around 200 unique postcodes which could be converted into the dummy variables but then very precious information of a particular location would be lost.</p> <p><strong>What do you do when you are given a new address?</strong> &lt;/br&gt; You either google where it is or how to get there.</p> <p>By using an external API following the four additional features given, the apartment’s address could be calculated:</p> <ol> <li>duration of a train trip to the S-Bahn Friedrichstrasse (central station)</li> <li> <p>distance to U-Bahn Stadtmitte (city center) by car</p> </li> <li> <p>duration of a walking trip to the nearest metro station</p> </li> <li>number of metro stations within one kilometer from the apartment</li> </ol> <p>These four features boosted the performance significantly.</p> <p><strong>Common mistakes when carrying out machine learning and data science</strong> After scraping or getting the data, there are many steps to accomplish before applying a machine learning model.</p> <p>You need to visualize each of the variables to see distributions, find the outliers, and understand why there are such outliers.</p> <p>What can you do with missing values in certain features?</p> <p>What would be the best way to convert categorical features into numerical ones?</p> <p>There are many such questions, but I will give some details on the ones where the majority of beginners encounter mistakes.</p> <h4 id="1-visualization">1. Visualization</h4> <p>Firstly, you should visualize the distribution of the continuous features to get a feeling if there are many outliers, what the distribution would be, and if it makes sense.</p> <p>There are many ways to visualize it, for example box plots, histograms, cumulative distribution functions, and violin plots. However, one should pick the plot that will give the most information about the data.</p> <p>To see the distribution (if it is normal, or bimodal), the histograms will be the most helpful. Although histograms are a good starting point, the box plots might be superior in identifying the number of outliers and seeing where the median quartiles lie.</p> <p>Based on the plots, the most interesting question would be: do you see what you expected to see? Answering this question will help you either in finding insights or finding bugs in the data.</p> <p>To get inspired and understand what plot will give the most value, I frequently referred to the Python’s <code class="language-plaintext highlighter-rouge">seaborn</code> gallery. Another good source of inspiration for the visualization and finding insights are kernels on <code class="language-plaintext highlighter-rouge">Kaggle</code>. Here is my <code class="language-plaintext highlighter-rouge">kaggle</code> kernel of the in-depth visualization of the titanic dataset.</p> <p>In the context of rental prices, I plotted the histograms of each continuous feature and expected to see a long right tail in the distribution of the rent without bills and total area.</p> <p>Box plots helped me see the number of outliers for each of the features. In fact, most of the outliers apartments based on the rent without bills were either the ateliers for the small shops with more than 200m2 or the student dormitories with very low rent.</p> <h4 id="2-do-i-impute-the-values-based-on-the-whole-dataset">2. Do I impute the values based on the whole dataset?</h4> <p>Sometimes there will be missing values, due to various reasons. If we exclude every observation with at least one missing value, we can end up with a very reduced dataset.</p> <p>There are many ways of imputing the values, mean, or median. It is up to you how to do it but make sure to calculate the imputation statistics only on the training data to avoid data leakage of your test set.</p> <p>In the rental data, I also extracted a description of the apartment. Whenever the quality, condition, or type of apartment was missing, I would impute it from the description if the description contained this information.</p> <h4 id="3-how-do-i-transform-categorical-variables">3. How do I transform categorical variables?</h4> <p>Some algorithms, depending on the implementation, wouldn’t work directly with the categorical data, so one would need to somehow transform them into numerical values.</p> <p>There are many ways of transforming categorical variables into numerical features, such as Label Encoder, One Hot Encoding, bin encoding, and hashing encoding. However, most people use the Label Encoding incorrectly when the One Hot Encoding should have been used instead.</p> <p>Assume, in our rental data, that we have an apartment-type column with the following values: [ground floor, loft, maisonette, loft, loft, ground floor]. LabelEncoder can turn this into [3,2,1,2,2,1], introducing ordinality, which means that ground_floor &gt;loft &gt; maisonette. For some algorithms like decision trees, and its deviations, this type of encoding for this feature would be fine, but applying regressions and SVM might not make that much sense.</p> <h4 id="4-do-i-need-to-standardize-variables">4. Do I need to standardize variables?</h4> <p>Standardization brings all continuous variables to the same scale, meaning if one variable has values from 1K to 1M and another from 0.1 to 1, after standardization they will have the same range.</p> <p>L1 or L2 regularizations are the common way of reducing overfitting and can be used within many regression algorithms. However, it is important to apply feature standardization before L1 or L2.</p> <p>The rental price is in Euros so the fitted coefficient would be approximately 100 times larger than the fitted coefficient if the price was in cents. L1 and L2 penalize the larger coefficients more, meaning it will penalize the features in smaller scales more. To prevent this, the features should be standardized before applying L1 or L2.</p> <p>Another reason to standardize is that if you or the your algorithm use gradient descent, gradient descent converges much faster with feature scaling.</p> <h4 id="5-do-i-need-to-derive-the-logarithm-of-the-target-variable">5. Do I need to derive the logarithm of the target variable?</h4> <p>It took me a while to understand that there is no universal answer.</p> <p>It depends on many factors:</p> <p>whether you want fractional or absolute error which algorithm you use what residual plots and changes in the metrics tell you In regression, firstly pay attention to the residual plots and the metric. Sometimes the logarithmization of the target variable leads to a better model and the results of the model would still be easy to understand. However, there are still other transformations that could be of interest, such as to taking the square root.</p> <p>There are many answers on Stack Overflow regarding this question, and I think Residual Plots and RMSE on raw and log target variable explains it very well.</p> <p>For the rental data, I derived the logarithm of the price as the residual plots looked a bit better.</p> <h4 id="6-some-more-important-stuff">6. Some more important stuff</h4> <p>Some algorithms, such as regressions, will suffer from collinearities in the data because the coefficients become very unstable (more math). SVM might or might not suffer from collinearity due to the choice of kernel.</p> <p>Decision-based algorithms will not suffer from multicollinearity as they could use features interchangeably in different trees without it affecting the performance. However, the interpretation of feature importance then gets more difficult as the correlated variable may not appear to be as important as it is.</p> <h3 id="machine-learning">Machine Learning</h3> <p>After you have familiarized yourself with data and cleaned out the outliers, it is the perfect time to get the hang of machine learning. There are many algorithms you could use for this supervised machine learning.</p> <p>There were three different algorithms I wanted to explore, comparing characterstics such as performance differences and speed. These three were gradient boosted trees with different implementations (XGBoost and LightGMB), Random Forest (FR, scikit-learn) and 3-layer Neuronal Networks (NN, Tensorflow). I selected RMSLE (root mean squared logarithm error) to be the metric for the optimization of the process. I used RMSLE because I derived the logarithm of the target variable.</p> <p>XGBoost and LigthGBM performed comparably, RF slightly worse, whereas NN was the worst.</p> <p>Decision tree-based algorithms are very good at interpreting features. For example, they produce a feature importance score.</p> <h4 id="feature-importance-finding-the-drivers-of-the-rental-price">Feature importance: finding the drivers of the rental price</h4> <p>After fitting a decision tree-based model, you can see what features are the most valuable for the price prediction.</p> <p>Feature importance provides a score that indicates how informative each feature was in the construction of the decision trees within the model. One of the ways to calculate this score is to count how many times a feature is used to split the data across all trees. This score can be computed in different ways.</p> <p>Feature importance can reveal other insights about the main price drivers.</p> <p>For the rental price prediction, it isn’t surprising that total area is the most important driver of the price. Interestingly, some features that were engineered with external API are also in the top most important features.</p> <p>However, as mentioned in “Interpretable Machine Learning with XGBoost”, there can be inconsistencies in feature importance depending on the attribution option. The author of the linked blogpost, and SHAP NIPS paper, proposes a new way of calculating feature importance that will be both accurate and consistent. This uses the shap Python library. SHAP values represent the responsibility of a feature for a change in the model output.</p> <h4 id="ensemble-averaging">Ensemble averaging</h4> <p>After playing around with different models and comparing performance, you could just combine the results of each of the model and build an ensemble!</p> <p>Bagging is the machine learning ensemble model that utilizes the predictions of several algorithms to calculate the final aggregated predictions. It is designed to prevent overfitting and reduces the variance of the algorithms.</p> <p>As I already had predictions from the above mentioned algorithms, I combined all four models in all possible ways and picked the seven best single and ensemble models based on the RMSLE of the validation set.</p> <p>The ensemble of three decision-tree based algorithms performed the best compared to each single model.</p> <p>You could also produce a weighted ensemble, assigning more weight to a better single model. The reasoning behind it is that other models could overrule the best model only if they collectively agree on an alternative.</p> <p>In reality, one would never know if an averaged ensemble would be better than the single model without just trying it out.</p> <h4 id="stacked-models">Stacked models</h4> <p>An averaged or weighted ensemble is not the only way to combine the predictions of different models. You could also stack the models in very different ways!</p> <p>The idea behind stacked models is to create several base models and a meta model on top of the results from the base models in order to produce final predictions. However, it is not so obvious how to train the meta model because it can be biased towards the best of the base models. A very good explanation of how to do it correctly can be found in the post “Stacking models for improved predictions”.</p> <p>For the rental price case, stacked models didn’t improve the RMSLE at all — they even increased the metrics. There might be several reasons for this — either I coded it incorrectly ;) or there was just too much noise introduced by stacking.</p> <p>If you want to explore more of the ensemble and stacked model articles, the Kaggle Ensemble Guide explains many different kinds of ensembling with the performance comparison and referrals on how such stacked models got to the top of Kaggle’s competitions.</p> <h3 id="final-thoughts">Final thoughts</h3> <ul> <li>listen to what people talk about around you; their complaining can serve as a good starting point for solving something big</li> <li>let people find their own insights by providing interactive dashboards don’t restrict yourself to common feature engineering as multiplying two variables</li> <li>Try to find additional sources of data or explanations</li> <li>try out ensembles and stacked models as those methods could improve the performance</li> </ul> <p>And please, provide the date of the data you display!</p> <h3 id="reference">Reference</h3> <p>Kokatjuhha, J. (2020, March 19). How to build a data science project from scratch. We’ve Moved to FreeCodeCamp.org/News. https://medium.com/free-code-camp/how-to-build-a-data-science-project-from-scratch-dc4f096a62a1</p> <p>‌</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/myblog/2023/review-machine-learning/">Basic Concepts of Machine Learning</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/myblog/2023/error_exception_python/">Error and Exception in Python</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/myblog/2023/performance-measures/">Performance Metrics in Machine Learning</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/myblog/2023/intro_pytorch/">Introduction to PyTorch</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/myblog/2023/eda/">Exploratory Data Analysis</a> </li> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 Duc Minh Quang Nguyen. # Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. # Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. # Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?07b8786bab9b4abe90d10e61f7d12ff7" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>