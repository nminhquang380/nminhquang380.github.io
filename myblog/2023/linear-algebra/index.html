<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Linear Algebra for AI | Duc Minh Quang Nguyen</title> <meta name="author" content="Duc Minh Quang Nguyen"> <meta name="description" content="# A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://nminhquang380.github.io/myblog/2023/linear-algebra/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Duc Minh Quang </span>Nguyen</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Portfolio</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">Resume</a> </li> <li class="nav-item "> <a class="nav-link" href="/bucket_list/">My Bucket List</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Linear Algebra for AI</h1> <p class="post-meta">June 21, 2023</p> <p class="post-tags"> <i class="fa-solid fa-calendar fa-sm"></i> 2023   ·   <i class="fa-solid fa-tag fa-sm"></i> deeplearning   </p> </header> <article class="post-content"> <div id="markdown-content"> <h2 id="1-scalars-vectors-matrices-and-tensors">1. Scalars, Vectors, Matrices and Tensors</h2> <p>The study of linear algebra involves several types of mathematical objects:</p> <ul> <li> <strong>Scalars</strong>: A scalar is just a single number, in contrast to most of the other objects studied in linear algebra, which are usually arrays of multiple numbers.</li> <li> <strong>Vectors</strong>: A vector is an array of numbers. The numbers are arranged in order. We can identify each individual number by its index in that ordering.</li> <li> <strong>Matrices</strong>: : A matrix is a 2-D array of numbers, so each element is identified by two indices instead of just one.</li> <li> <strong>Tensors</strong>: In some cases we will need an array with more than two axes. In the general case, an array of numbers arranged on a regular grid with a variable number of axes is known as a tensor</li> </ul> <p>One important operation on matrices is the transpose. The transpose of a matrix is the mirror image of the matrix across a diagonal line, called the main diagonal, running down and to the right, starting from its upper left corner. We denote the transpose of a matrix A as $A^T$, and it is defined such that: \((A^T)_{i,j}=A_{j,i}\)</p> <p>In the context of deep learning, we also use some less conventional notation. We allow the addition of a matrix and a vector, yielding another matrix: $C = A + b$, where $C_{i,j} = A_{i,j} + b_j$ . In other words, the vector b is added to each row of the matrix. This shorthand eliminates the need to define a matrix with b copied into each row before doing the addition. This implicit copying of b to many locations is called <strong>broadcasting</strong>.</p> <h2 id="2-multiplying-matrices-and-vectors">2. Multiplying Matrices and Vectors</h2> <p>One of the most important operations involving matrices is multiplication of two matrices. The matrix product of matrices A and B is a third matrix C. In order for this product to be defined, A must have the same number of columns as B has rows. If A is of shape m × n and B is of shape n × p, then C is of shape m × p. We can write the matrix product just by placing two or more matrices together, for example, \(C = AB\) Note that the standard product of two matrices is not just a matrix containing the product of the individual elements. Such an operation exists and is called the element-wise product, or Hadamard product, and is denoted as $A \odot B$.</p> <h2 id="3-identity-and-inverse-matrices">3. Identity and Inverse Matrices</h2> <p>To describe matrix inversion, we first need to define the concept of an identity matrix. An identity matrix is a matrix that does not change any vector when we multiply that vector by that matrix. We denote the identity matrix that preserves n-dimensional vectors as $I_n$. \(\forall x \in \R^n, I_nx = x\) The <strong>matrix inverse</strong> of A is denoted as $A^{-1}$, and it is defined as the matrix such that \(A^{-1}A = I_n\)</p> <h2 id="4-norms">4. Norms</h2> <p>Sometimes we need to measure the size of a vector. In machine learning, we usually measure the size of vectors using a function called a norm. Formally, the $L^p$ norm is given by \(||x||_p = (\sum_i {|x_i|^p})^-\) The L2 norm, with p = 2, is known as the <strong>Euclidean norm</strong>, which is simply the Euclidean distance from the origin to the point identified by x.</p> <p>The L1 norm is commonly used in machine learning when the difference between zero and nonzero elements is very important.</p> <p>Sometimes we may also wish to measure the size of a <strong>matrix</strong>. In the context of deep learning, the most common way to do this is with the otherwise obscure <strong>Frobenius norm</strong>: \(||A||_F = \sqrt{\sum_{i,j}A_{i,j}^2}\)</p> <h2 id="5-special-kinds-of-matrices-and-vectors">5. Special Kinds of Matrices and Vectors</h2> <ol> <li> <strong>Diagonal matrices</strong> consist mostly of zeros and have nonzero entries only along the main diagonal.</li> <li>A <strong>symmetric matrix</strong> is any matrix that is equal to its own transpose: $A = A^T$.</li> <li> <table> <tbody> <tr> <td>A <strong>unit vector</strong> is a vector with unit norm: $</td> <td> </td> <td>x</td> <td> </td> <td>_2=1$.</td> </tr> </tbody> </table> </li> <li>An <strong>orthogonal matrix</strong> is a square matrix whose rows are mutually orthonormal and whose columns are mutually orthonormal: \(A^T A = A A^T = I.\) This implies that \(A^{-1} = A^T\) <h2 id="6-eigendecomposition">6. Eigendecomposition</h2> <p>One of the most widely used kinds of matrix decomposition is called eigendecomposition, in which we decompose a matrix into a set of eigenvectors and eigenvalues. An eigenvector of a square matrix A is a nonzero vector v such that multi- plication by A alters only the scale of v: \(Av = \lambda v\) The scalar λ is known as the eigenvalue corresponding to this eigenvector. Suppose that a matrix A has n linearly independent eigenvectors {v (1) , . . . , v (n)} with corresponding eigenvalues {λ1, . . . , λn}. We may concatenate all the eigenvectors to form a matrix V with one eigenvector per column: V = [v (1) , . . . , v (n) ]. Likewise, we can concatenate the eigenvalues to form a vector λ = [λ1 , . . . ,λn]T. The eigendecomposition of A is then given by \(A = Vdial(\lambda)V^{-1}\) We have seen that constructing matrices with specific eigenvalues and eigen- vectors enables us to stretch space in desired directions. Yet we often want to decompose matrices into their eigenvalues and eigenvectors. Doing so can help us analyze certain properties of the matrix, much as decomposing an integer into its prime factors can help us understand the behavior of that integer.</p> </li> </ol> <p>Specifically, every real symmetric matrix can be decomposed into an expression using only real-valued eigenvectors and eigenvalues:</p> \[A = Q Q^T\] <p>where Q is an orthogonal matrix composed of eigenvectors of A, and is a diagonal matrix. The eigenvalue Λi,i is associated with the eigenvector in column i of Q, denoted as Q:,i. Because Q is an orthogonal matrix, we can think of A as scaling space by λi in direction v(i).</p> <h2 id="7-singular-value-decomposition">7. Singular Value Decomposition</h2> <p>The SVD enables us to discover some of the same kind of information as the eigendecomposition reveals; however, the SVD is more generally applicable. Every real matrix has a singular value decomposition, but the same is not true of the eigenvalue decomposition. For example, if a matrix is not square, the eigendecomposition is not defined, and we must use a singular value decomposition instead.</p> <p>The singular value decomposition is similar, except this time we will write A as a product of three matrices:</p> \[A = UDV^T\] <p>Suppose that A is an m ×n matrix. Then U is defined to be an m ×m matrix, D to be an m × n matrix, and V to be an n × n matrix.</p> <h2 id="8-the-moore-penrose-pseudoinverse">8. The Moore-Penrose Pseudoinverse</h2> <p>Practical algorithms for computing the pseudoinverse are based not on this defini- tion, but rather on the formula</p> \[A^+ = V D^+U^T\] <p>where U, D and V are the singular value decomposition ofA, and the pseudoinverse D+ of a diagonal matrix D is obtained by taking the reciprocal of its nonzero elements then taking the transpose of the resulting matrix.</p> <h2 id="9-the-trace-operator-and-determinant">9. The Trace Operator and Determinant</h2> <p>The trace operator gives the sum of all the diagonal entries of a matrix:</p> \[Tr(A) = \sum_{i}A_{i,j}\] <p>The trace operator is useful for a variety of reasons. Some operations that are difficult to specify without resorting to summation notation can be specified using matrix products and the trace operator. For example, the trace operator provides an alternative way of writing the Frobenius norm of a matrix: \(||A||_F = \sqrt{Tr(AA^T)}\)</p> <p>The determinant of a square matrix, denoted det(A), is a function that maps matrices to real scalars. The determinant is equal to the product of all the eigenvalues of the matrix.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/myblog/2023/error_exception_python/">Error and Exception in Python</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/myblog/2023/build-ds-project/">Get lost when trying to build a Data Science Project</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/myblog/2023/intro_pytorch/">Introduction to PyTorch</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/myblog/2023/performance-measures/">Performance Metrics in Machine Learning</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/myblog/2023/proba-n-info-theory/">Probability and Information Theory</a> </li> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2023 Duc Minh Quang Nguyen. # Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. # Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. # Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?07b8786bab9b4abe90d10e61f7d12ff7" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>